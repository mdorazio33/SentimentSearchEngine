{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4caacd9-cf6b-48a1-a216-47f8c542c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d1579-4f56-4b7f-a988-00b9734fd61c",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6acd61-dceb-41b4-82e9-ee3b0cb28ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b430033-ae82-4a35-9e58-6d690fc900ec",
   "metadata": {},
   "source": [
    "### Converting entries in Comment column to Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdd4ddf-3eae-47a1-ab12-24b01efadad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    when modi promised a minimum government maximu...\n",
      "1    talk all the nonsense and continue all the dra...\n",
      "2    what did just say vote for modi  welcome bjp t...\n",
      "3    asking his supporters prefix chowkidar their n...\n",
      "4    answer who among these the most powerful world...\n",
      "Name: Comment, dtype: object\n",
      "Comment\n",
      "<class 'str'>      269652\n",
      "<class 'float'>       105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Comment\"].head())\n",
    "print(df[\"Comment\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3833606-f05a-49f5-95a4-6827ef3c9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Comment\"] = df[\"Comment\"].fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3817073d-6cec-40aa-ad2e-afe108599257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    when modi promised a minimum government maximu...\n",
      "1    talk all the nonsense and continue all the dra...\n",
      "2    what did just say vote for modi  welcome bjp t...\n",
      "3    asking his supporters prefix chowkidar their n...\n",
      "4    answer who among these the most powerful world...\n",
      "Name: Comment, dtype: object\n",
      "Comment\n",
      "<class 'str'>    269757\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Comment\"].head())\n",
    "print(df[\"Comment\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb92ab56-af8e-4bcb-a456-0be9c5f284c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d91c05d-d117-4921-9ee0-05c42cac185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].isnull().sum())  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78dfa64-127b-422d-b15f-57e41acbb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30abd0fc-8e81-41ca-a873-235eb4d9f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['Sentiment'].isnull().sum())  # Count missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7529bbfa-6aa4-4a0d-a953-ab1ec6429ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401abcc-1b91-460a-82f0-b5d6cb6ed1fb",
   "metadata": {},
   "source": [
    "### Mapping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56034a3-43b5-4482-bf61-163d65a4f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 269750/269750 [00:07<00:00, 34152.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def map_labels(example):\n",
    "    if example[\"Sentiment\"] == -1:\n",
    "        example[\"Sentiment\"] = 0  # Negative\n",
    "    elif example[\"Sentiment\"] == 0:\n",
    "        example[\"Sentiment\"] = 1  # Neutral\n",
    "    elif example[\"Sentiment\"] == 1:\n",
    "        example[\"Sentiment\"] = 2  # Positive\n",
    "    return example\n",
    "\n",
    "mapped_dataset = dataset.map(map_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9540320-bca8-49e8-9171-75abc0b28cb8",
   "metadata": {},
   "source": [
    "### Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d812883-e7d8-488d-a0d3-c072928fab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 269750/269750 [01:22<00:00, 3272.37 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 215800/215800 [00:06<00:00, 33309.00 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53950/53950 [00:01<00:00, 34073.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Comment\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = mapped_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Ensure the labels are properly added\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']\n",
    "\n",
    "# Map the labels to ensure they are integers\n",
    "train_dataset = train_dataset.map(lambda x: {'labels': torch.tensor(x['Sentiment'], dtype=torch.long)}, batched=True)\n",
    "val_dataset = val_dataset.map(lambda x: {'labels': torch.tensor(x['Sentiment'], dtype=torch.long)}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04276905-657b-4915-a2c4-a2fbf5e24b5b",
   "metadata": {},
   "source": [
    "### Loading BERT (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51185238-58af-41b1-a0c3-8b810ba98544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1877b-5be8-4afe-925d-ee18e6356d47",
   "metadata": {},
   "source": [
    "### Defining our Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e52e52-bb41-4d2f-8a59-42b2710ab680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77408a-9f33-43c4-9571-a972de268e3a",
   "metadata": {},
   "source": [
    "### Defining the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15155d18-e804-4697-96c6-c3e5e3cc4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35114fad-369e-48df-b6a2-195fa3b02930",
   "metadata": {},
   "source": [
    "### Initializing the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766b4b15-9d75-4867-9352-af576d7a29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Get the labels from the inputs\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # Convert labels to long type as CrossEntropyLoss expects long tensor for labels\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)  # This gets the logits\n",
    "        logits = outputs.logits  # Extract logits from the model's output\n",
    "        \n",
    "        # Use CrossEntropyLoss, which takes logits of shape [batch_size, num_classes] and labels of shape [batch_size]\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)  # Pass logits and labels directly to the loss function\n",
    "        \n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6854eff-cc3f-4bff-8365-28c43a783429",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b897893-16e7-4e93-a50a-5508ee381c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,  \n",
    "    data_collator=data_collator, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc07915-5d2c-41fc-a877-a6554cad2962",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea72802-4f34-497e-b901-4d0bd58c5f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40464' max='40464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40464/40464 23:21:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221200</td>\n",
       "      <td>0.210430</td>\n",
       "      <td>0.929175</td>\n",
       "      <td>0.929472</td>\n",
       "      <td>0.930209</td>\n",
       "      <td>0.929175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.188503</td>\n",
       "      <td>0.942428</td>\n",
       "      <td>0.942600</td>\n",
       "      <td>0.943038</td>\n",
       "      <td>0.942428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.205120</td>\n",
       "      <td>0.947322</td>\n",
       "      <td>0.947287</td>\n",
       "      <td>0.947268</td>\n",
       "      <td>0.947322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40464, training_loss=0.19069658165266196, metrics={'train_runtime': 84066.529, 'train_samples_per_second': 7.701, 'train_steps_per_second': 0.481, 'total_flos': 2.14402308217344e+16, 'train_loss': 0.19069658165266196, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c7205-24ba-4a34-9c6e-5e9af42a6719",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f253d04e-a8b2-44c8-885f-808e0661b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3372' max='3372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3372/3372 28:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.18850284814834595, 'eval_accuracy': 0.9424281742354031, 'eval_f1': 0.9426000387059595, 'eval_precision': 0.9430379900320718, 'eval_recall': 0.9424281742354031, 'eval_runtime': 1727.906, 'eval_samples_per_second': 31.223, 'eval_steps_per_second': 1.951, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8db46d-5c1d-449d-a5c2-5c6527f94c0b",
   "metadata": {},
   "source": [
    "### Saving the Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4ad6d39-0246-4af2-8473-dc16f74fb8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./db_model\\\\tokenizer_config.json',\n",
       " './db_model\\\\special_tokens_map.json',\n",
       " './db_model\\\\vocab.txt',\n",
       " './db_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./db_model\")\n",
    "tokenizer.save_pretrained(\"./db_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
